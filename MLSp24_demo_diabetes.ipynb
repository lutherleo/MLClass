{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lutherleo/MLClass/blob/main/MLSp24_demo_diabetes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXolYgnBy3mb"
      },
      "source": [
        "# Demo:  Predicting Diabetes Progression using Mulitple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1udZYN1y3mc"
      },
      "source": [
        "In this demo, you will learn how to:\n",
        "* Fit multiple linear regression models, by hand and using Python's Scikit-Learn package.  \n",
        "* Split data into training and test set.\n",
        "* Manipulate and visualize multivariable arrays.\n",
        "\n",
        "We first load the packages as usual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8aSS9rPy3md"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsVYxV2Vy3me"
      },
      "source": [
        "We will also load a few modules from [scikit-learn](https://scikit-learn.org/stable/), which is one of the most popular machine learning libraries in Python that does not focus specifically on neural nets and deep learning. Using the libary is a bit overkill at this point, but I want you to get some practice because it will be very useful later in the course, and for your mini-projects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uMDI1w8y3me"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets, linear_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQj7GDyuy3mf"
      },
      "source": [
        "## Diabetes Data Example\n",
        "To illustrate the concepts, we load a well-known diabetes data set.  This dataset can be downloaded in raw form from [https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html](https://www4.stat.ncsu.edu/~boos/var.select/diabetes.html). However, it is also included in the `sklearn.datasets` module, which prepackages a number of benchmark datasets for easy experimentation. It can be loaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1z0hriLSy3mf"
      },
      "outputs": [],
      "source": [
        "# Load the diabetes dataset\n",
        "diabetes = datasets.load_diabetes()\n",
        "X = diabetes.data\n",
        "y = diabetes.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQwlQCIky3mg"
      },
      "source": [
        "It's always good to check what type of objects we have loaded in. You can confirm using the `type` function that our data is preloaded into `numpy` arrays, which is how we want it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKy05dxzy3mh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02ad5af6-8526-4238-aba2-166f7296bff4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "type(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okxTzTW8y3mi"
      },
      "source": [
        "The predictor variables are **age**, **sex**, **body mass index**, **average blood pressure**, and **six blood serum measurements** (think iron, cholesterol, lipid levels, etc.). Each corresponds to a column in the data matrix `X`. Note that these columns have been *mean centered* and had their *variance normalized* (i.e. scaled). This will not effect your linear regression model at all, but it's good to know."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SX6mSVEny3mi"
      },
      "source": [
        "When you look at the first 5 entries of the **age** column, we don't see age numbers we might expect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3Losc8LLy3mj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140064ed-3631-4e35-f516-5a1bbb9c9715"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.03807591, -0.00188202,  0.08529891, -0.08906294,  0.00538306])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "X[0:5,0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Pklc67_y3mk"
      },
      "source": [
        "And when you look at the first 5 entries of the **sex** column, we don't see binary categories like 1 for female, 2 for male. We see real numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "YOK5CNL3y3mk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d225373a-7bb7-42c7-a2ec-13f3141d3a85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.05068012, -0.04464164,  0.05068012, -0.04464164, -0.04464164])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "X[0:5,1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTI9VrH3y3mk"
      },
      "source": [
        "**NOTE**: This is one reason to be cautious about using built in datasets. Often a lot of information is lost during preprocessing which makes it harder to reason about the data than if you were working with the raw files. This is just a simple teaching demo, so we don't care for today, but **keep that in mind** when downloading data for your projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLSFzyMpy3ml"
      },
      "source": [
        "The target values `y` represent a \"quantitative measure of diabetes disease progression\" that we wish to predict. The exact meaning of this measure seems to have been lost in data translation, but we will work with it anyway.\n",
        "\n",
        "The number of attributes and samples are computed from the shape:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wJQzMfqWy3ml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eec96d3e-041a-419f-9353-49ff7255a00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num samples=442 num attributes=10\n"
          ]
        }
      ],
      "source": [
        "nsamp, natt = X.shape\n",
        "print(\"num samples=\"+str(nsamp)+\" num attributes=\"+str(natt))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfqN_srjy3mm"
      },
      "source": [
        "## Using Simple Linear Regression for Each Feature Individually\n",
        "\n",
        "As a first attempt to predict diabetes progression, we could try *one attribute at a time*.  That is, for each attribute $x_k$, we could attempt to fit a simple linear regression model:\n",
        "$$ y \\approx \\beta_{0,k} + \\beta_{1,k}x_k$$\n",
        "where $\\beta_{0,k}$ and $\\beta_{1,k}$ are the coefficients in the simple linear regression model using only the attribute $x_k$.\n",
        "\n",
        "The following code computes the $\\ell_2$ loss for each variable $k$ as well as the coefficients in the linear model, $\\beta_{0,k}$ and $\\beta_{1,k}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "y1mUQbSLy3mm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f55cc5b8-db60-4fb9-dd6d-4f4bf2dac169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss=2.53e+06 beta0=152.13348416289605 beta1=304.1830745282947\n",
            "1 loss=2.62e+06 beta0=152.13348416289594 beta1=69.71535567841465\n",
            "2 loss=1.72e+06 beta0=152.1334841628967 beta1=949.435260383949\n",
            "3 loss=2.11e+06 beta0=152.13348416289585 beta1=714.7416437042876\n",
            "4 loss=2.50e+06 beta0=152.13348416289597 beta1=343.2544518889643\n",
            "5 loss=2.54e+06 beta0=152.1334841628959 beta1=281.7845933524592\n",
            "6 loss=2.21e+06 beta0=152.13348416289566 beta1=-639.1452793225127\n",
            "7 loss=2.14e+06 beta0=152.13348416289568 beta1=696.8830300922431\n",
            "8 loss=1.78e+06 beta0=152.13348416289628 beta1=916.1387228150982\n",
            "9 loss=2.24e+06 beta0=152.13348416289614 beta1=619.2228206843339\n"
          ]
        }
      ],
      "source": [
        "ym = np.mean(y)\n",
        "losses = np.zeros(natt)\n",
        "beta0 = np.zeros(natt)\n",
        "beta1 = np.zeros(natt)\n",
        "for k in range(natt):\n",
        "    xm = np.mean(X[:,k])\n",
        "    sxy = np.mean((X[:,k]-xm)*(y-ym))\n",
        "    sxx = np.mean((X[:,k]-xm)**2)\n",
        "    beta1[k] = sxy/sxx\n",
        "    beta0[k] = ym - beta1[k]*xm\n",
        "    errs = y - beta1[k]*X[:,k] - beta0[k]\n",
        "    losses[k] = np.sum(errs**2)\n",
        "\n",
        "    print(str(k)+\" loss=\"+\"{:.2e}\".format(losses[k])+\" beta0=\"+str(beta0[k])+\" beta1=\"+str(beta1[k]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cK7Zab1Ay3mm"
      },
      "source": [
        "**Test yourself:** Why are all the `beta0` values the same, no matter what predictor we use? This is not a bug!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzcRgNzXy3mm"
      },
      "source": [
        "We see that the best prediction is given by BMI index, with loss 1.72e+06. If you convert this to an $R^2$ value, as described in class, we get a value of $R^2 = .344$, which on a scale from $[0,1]$ does not indicate too strong of a fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgOl70kwy3mn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd2eef2f-951e-4a61-9e56-4c72c67afb0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rsqr=0.344\n"
          ]
        }
      ],
      "source": [
        "syy = np.mean((y-ym)**2)\n",
        "rsqr = 1 - np.min(losses)/(nsamp*syy)\n",
        "print(\"Rsqr=\"+\"{:.3}\".format(rsqr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_MyJYcHy3mn"
      },
      "source": [
        "We can see this somewhat poor fit in a scatter plot as well where there is significant variation from the regression line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "x02D_I8Xy3mo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "9e4cfd2d-5f75-487f-af5d-fb91decb24b1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Find the index of the single variable with the best squared loss\n",
        "imax = np.argmin(losses)\n",
        "\n",
        "# Regression line over the range of x values\n",
        "xmin = np.min(X[:,imax])\n",
        "xmax = np.max(X[:,imax])\n",
        "ymin = beta0[imax] + beta1[imax]*xmin\n",
        "ymax = beta0[imax] + beta1[imax]*xmax\n",
        "# this is a different approach to plotting a regression line then we used in the last demo\n",
        "plt.plot([xmin,xmax], [ymin,ymax], 'r-', linewidth=3)\n",
        "\n",
        "# Scatter plot of points\n",
        "plt.scatter(X[:,imax],y)\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJM0FzOyy3mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24928028-e55f-4038-8157-333f1e1a9f06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "235"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "np.sum(X[:,1] <0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvzWOLFyy3mo"
      },
      "source": [
        "Note that the calculations above to find the simple linear regression parameters could have been done without a for-loop as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-bCXCNRy3mo"
      },
      "outputs": [],
      "source": [
        "# Remove the means\n",
        "ym = np.mean(y)\n",
        "y1 = y-ym  # a column vecotor each minus mean\n",
        "Xm = np.mean(X,axis=0) # averaging over column, resulting a row vector of dimension natt\n",
        "X1 = X - Xm[None,:] # minus the same mean in each column\n",
        "\n",
        "# Compute the correlations per features\n",
        "Sxx = np.mean(X1**2,axis=0) #a row vector with each element indicating the variance of one attribute\n",
        "Sxy = np.mean(X1*y1[:,None],axis=0) #a row vector with each element indicating the covarance on one attribute to the targer\n",
        "\n",
        "# Compute the coefficients and losses per feature\n",
        "beta1 = Sxy/Sxx # element wise division, resulting a row vector containing  beta1 for each attribute\n",
        "beta0 = ym - beta1*Xm # element wise multiplication, resulting a row vector containing beta0 for each attribute\n",
        "errs = ((X*beta1) + beta0) - y[:,None] # results in a matrix where every column contains the residuals for predictor k\n",
        "losses = np.sum(errs**2,axis=0) # sums up the squared size of the residuals across each column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYtibg9sy3mp"
      },
      "source": [
        "Now let us see whether we get the same result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "qVryAMc0y3mp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b3cb8a6-229d-4ecb-da43-c07176f9cf3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss=2.53e+06 beta0=152.13348416289605 beta1=304.1830745282949\n",
            "1 loss=2.62e+06 beta0=152.13348416289594 beta1=69.71535567841437\n",
            "2 loss=1.72e+06 beta0=152.1334841628967 beta1=949.4352603839499\n",
            "3 loss=2.11e+06 beta0=152.13348416289585 beta1=714.7416437042887\n",
            "4 loss=2.50e+06 beta0=152.13348416289597 beta1=343.2544518889645\n",
            "5 loss=2.54e+06 beta0=152.1334841628959 beta1=281.7845933524593\n",
            "6 loss=2.21e+06 beta0=152.13348416289566 beta1=-639.145279322514\n",
            "7 loss=2.14e+06 beta0=152.13348416289568 beta1=696.8830300922432\n",
            "8 loss=1.78e+06 beta0=152.13348416289628 beta1=916.1387228150995\n",
            "9 loss=2.24e+06 beta0=152.13348416289614 beta1=619.2228206843332\n"
          ]
        }
      ],
      "source": [
        "for k in range(natt):\n",
        "    print(str(k)+\" loss=\"+\"{:.2e}\".format(losses[k])+\" beta0=\"+str(beta0[k])+\" beta1=\"+str(beta1[k]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VYb2VRty3mq"
      },
      "source": [
        "## Improvements with a Multiple Variable Linear Model\n",
        "\n",
        "One possible way to try to improve the fit is to use multiple variables at the same time.\n",
        "\n",
        "We can manually compute the regression coefficients using the least-squares matrix formula given in class.\n",
        "\n",
        "To compute the coefficients manually, we first append an all ones columns onto `X` using the `ones` command and `hstack`.  Note that after we do this, `X` will have 11 columns -- one more column that original data matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ULztFiO8y3mq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65439426-ceb8-4e45-cfd6-3fad6cc611b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(442, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "ones = np.ones((nsamp,1))\n",
        "X_orig = X\n",
        "X = np.hstack((ones,X_orig))\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJL19s6Vy3mr"
      },
      "source": [
        "Next we need to solve $\\min_{\\mathbf{\\beta}}\\|\\mathbf{y} - \\mathbf{X\\beta}\\|_2^2$. The most obvious way to do so is via the direct matrix equation derived in class: $\\mathbf{\\beta}^* = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1m5kVE7Wy3mr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03d674d6-4258-4ab6-918a-8e0f4e768199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 152.13348416  -10.01219782 -239.81908937  519.83978679  324.39042769\n",
            " -792.18416163  476.74583782  101.04457032  177.06417623  751.27932109\n",
            "   67.62538639]\n"
          ]
        }
      ],
      "source": [
        "Xt = np.transpose(X)\n",
        "beta= np.linalg.inv(Xt@X)@Xt@y\n",
        "print(beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyhwT9MKy3mr"
      },
      "source": [
        "Note that `beta[0]` is the value of the intercept from the regression fit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6NFBW7Ny3ms"
      },
      "source": [
        "An often slightly faster alternative is to use the `lstsq` method, which solves $\\min_{\\mathbf{\\beta}}\\|\\mathbf{y} - \\mathbf{X\\beta}\\|_2^2$ using a QR decomposition. This should find the exact same least-squares fit. You might get a warning when you run `lstqr` about some defaults changing in the future. You can safetly ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Pp4VMI7ry3ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bb281cc-51da-4a8a-ce2f-cb154fd0f00f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 152.13348416  -10.01219782 -239.81908937  519.83978679  324.39042769\n",
            " -792.18416163  476.74583782  101.04457032  177.06417623  751.27932109\n",
            "   67.62538639]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
            "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ],
      "source": [
        "out = np.linalg.lstsq(X,y)\n",
        "beta = out[0]\n",
        "print(beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVnDyH2hy3ms"
      },
      "source": [
        "Finally, for very large problems (many examples and/or features), *iterative* methods can offer a much faster approach to solving $\\min_{\\mathbf{\\beta}}\\|\\mathbf{y} - \\mathbf{X\\beta}\\|_2^2$. There are several options avialable in the `scipy` library, including the popular LSQR method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQb93zFuy3mt"
      },
      "outputs": [],
      "source": [
        "import scipy as sp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJKlT7dAy3mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d69e680-07b7-4fb0-a67e-d89262848069"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 152.13348416  -10.01219324 -239.81908542  519.8397932   324.39043343\n",
            " -792.18415438  476.74584526  101.04456435  177.06418529  751.27932909\n",
            "   67.6253932 ]\n"
          ]
        }
      ],
      "source": [
        "out = sp.sparse.linalg.lsqr(X,y)\n",
        "beta = out[0]\n",
        "print(beta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ms8FARI5y3mt"
      },
      "source": [
        "Hopefully you got the same values of $\\mathbf{\\beta}$ no matter which algorithm you used. The differences are negligble because this demo problem is so small, but if you ever want to compare runtimes head-to-head, you can use the `time` library to time different operations. Here's an example that times the direct approach:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWB-TSYxy3mt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "718945e0-8fc3-4931-e6a0-e57949aa9abf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "direct matrix computation time: 0.00027251243591308594 seconds\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "t = time.time()\n",
        "beta= np.linalg.inv(Xt@X)@Xt@y\n",
        "elapsed = time.time() - t\n",
        "print(\"direct matrix computation time: \" + str(elapsed) + \" seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4gER5SKy3mt"
      },
      "source": [
        "Now let's compute the loss for our muliple variable model. Be careful with ndarrays dimensions! Always check what you're working with by running commands like `beta.shape`. Or multiplications may not behave as expected."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(beta.shape)"
      ],
      "metadata": {
        "id": "0cqogKbv5vDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92104a7b-a1c4-4750-9c83-fad3edd1df4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(442, 11)\n",
            "(11,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "4FdtHPley3mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d71e1a2-9e08-4ed9-d27b-6f3fe34d2d35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(442,)\n",
            "multiple variable loss=1.26e+06\n"
          ]
        }
      ],
      "source": [
        "yp = X@beta\n",
        "errs = (y - yp)\n",
        "print(errs.shape)\n",
        "lossm = np.sum(errs**2)\n",
        "\n",
        "print(\"multiple variable loss=\"+\"{:.2e}\".format(lossm))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "573TRkyry3mv"
      },
      "source": [
        "Note that this loss is about $30\\%$ lower than what we acheived with the best single variable linear regression, which is significant. Let's also normalize the loss to get an $R^2$ value, which we can compare to the value of $34\\%$ we obtained earlier. Higher $R^2$ is better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmDf1mVQy3mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f352588f-8e63-4eb9-8ce3-5cfc23530e81"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5177494254132934"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "rsqr = 1 - lossm/(nsamp*syy)\n",
        "rsqr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keqgb751y3mv"
      },
      "source": [
        "## Using built in sci-kit learn model\n",
        "\n",
        "We can also fit the multiple variable linear regression model using Sci-kit Learn's built in functionality. As mentioned, this is a bit overkill at this point, but it's good to see because you might use other built in models from `sklearn` later in the course. The disadvantage is that this approach leaves everything inside a \"black-box\". For example, how do you think this method is solving the required least-squares optimization problem? Using an iterative method like `lsqr`? Or a direct method?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS3Jof-gy3mv"
      },
      "source": [
        "To fit the linear model, we first create a regression object and then fit the data with regression object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFjUpzePy3mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e67ae2d0-4eee-4a2d-d16d-fd7c502b6ecf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "X = X[:,1:11]\n",
        "regr = linear_model.LinearRegression()\n",
        "regr.fit(X,y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMcCuzg6y3mw"
      },
      "source": [
        "You can see the intercept and other coefficients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0WMTXNky3mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41507acd-6f08-4a0e-c887-906c752a0b08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "152.1334841628965"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "regr.intercept_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "V5DkEfBry3mw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64566f22-aab9-4638-8ba6-7ff17b195599"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ -10.01219782, -239.81908937,  519.83978679,  324.39042769,\n",
              "       -792.18416163,  476.74583782,  101.04457032,  177.06417623,\n",
              "        751.27932109,   67.62538639])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "regr.coef_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbhLwI1qy3mw"
      },
      "source": [
        "We next compute the squared loss by using the `predict` function and you can verify that it's the same as above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCVFS_PGy3mx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f064025-50b1-40a2-9786-66b68ce5cfdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "multiple variable loss=1.26e+06\n"
          ]
        }
      ],
      "source": [
        "y_pred = regr.predict(X)\n",
        "lossm = np.linalg.norm(y_pred - y)**2\n",
        "\n",
        "print(\"multiple variable loss=\"+\"{:.2e}\".format(lossm))"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}